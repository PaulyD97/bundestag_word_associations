{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Data preparation and association matrix\n",
    "In this notebook the data (speeches) gets prepared. The punctuation gets removed, every word is set to lowercase, the stopwords get removed and the words get lemmatized.\n",
    "After that the association matrix is computed."
   ],
   "id": "720c88c047fd0c37"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-02T10:00:08.311221Z",
     "start_time": "2024-09-02T09:59:54.259646Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import jsonlines\n",
    "import json\n",
    "import copy\n",
    "import string\n",
    "import spacy\n",
    "nlp = spacy.load('de_core_news_sm')\n",
    "from collections import Counter\n",
    "from collections import defaultdict\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import tables\n",
    "import numpy as np"
   ],
   "id": "554bf73b5f4bea1b",
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-09-02T10:01:26.955178Z",
     "start_time": "2024-09-02T10:01:22.407263Z"
    }
   },
   "source": [
    "# load data in jsonfile\n",
    "speeches_19_20 = []\n",
    "with jsonlines.open('data/speeches_19.jsonl') as f:\n",
    "    for line in f.iter():\n",
    "        #for line in list(f):\n",
    "        speeches_19_20.append(line)\n",
    "with jsonlines.open('data/speeches_20.jsonl') as f:\n",
    "    for line in f.iter():\n",
    "        #for line in list(f):\n",
    "        speeches_19_20.append(line)"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-01T20:33:29.221745Z",
     "start_time": "2024-09-01T20:33:28.332910Z"
    }
   },
   "cell_type": "code",
   "source": "speeches_19_20_prep = copy.deepcopy(speeches_19_20)",
   "id": "c8bf1b4cc7e5bf1d",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-31T08:39:00.097360Z",
     "start_time": "2024-08-31T08:36:42.144655Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# delete punctuation\n",
    "punctuation = string.punctuation + '–' + '-' + '--' + '––'\n",
    "for rede in speeches_19_20_prep:\n",
    "    rede_dummy = \"\"\n",
    "    for char in rede['text']:\n",
    "        if char not in punctuation:\n",
    "            rede_dummy += char\n",
    "        rede['text'] = rede_dummy"
   ],
   "id": "98cdc9d2e7848b78",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-31T10:00:17.548952Z",
     "start_time": "2024-08-31T08:39:00.099394Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#lemmatization\n",
    "for rede in speeches_19_20_prep:\n",
    "    doc = nlp(rede['text'])\n",
    "    lemmatized_text = ' '.join([token.lemma_ for token in doc])\n",
    "    rede['text'] = lemmatized_text"
   ],
   "id": "d75d0807816d64db",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-31T10:00:19.048346Z",
     "start_time": "2024-08-31T10:00:17.564980Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# lower case\n",
    "for rede in speeches_19_20_prep:\n",
    "    rede['text'] = rede['text'].lower()"
   ],
   "id": "4ec25d6d20b20abc",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-31T11:25:29.988629Z",
     "start_time": "2024-08-31T10:00:19.048856Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#delete single and double char words\n",
    "def filter_single_double_char_words(text):\n",
    "    doc = nlp(text)\n",
    "    filtered_tokens = [token.text for token in doc if len(token.text) > 2]\n",
    "    return \" \".join(filtered_tokens)\n",
    "\n",
    "#apply funcion to each dictionary in the list\n",
    "for rede in speeches_19_20_prep:\n",
    "    rede[\"text\"] = filter_single_double_char_words(rede[\"text\"])"
   ],
   "id": "e961c3e0e057931",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-31T11:35:06.860571Z",
     "start_time": "2024-08-31T11:25:29.990661Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# stopwords with stopword list from github.com/solariz/german_stopwords\n",
    "\n",
    "file_stopwords = open('data/german_stopwords_full.txt','r', encoding ='utf-8')\n",
    "stopwords_list = file_stopwords.read().splitlines()\n",
    "file_stopwords.close()\n",
    "\n",
    "for rede in speeches_19_20_prep:\n",
    "    words = rede['text'].split()\n",
    "    filtered_words = [word for word in words if word not in stopwords_list]\n",
    "    rede['text'] = ' '.join(filtered_words)"
   ],
   "id": "9b2931f4d383b26c",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-31T11:35:10.618722Z",
     "start_time": "2024-08-31T11:35:06.862611Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# stopwords with own stopwords identified during the work on the project\n",
    "\n",
    "stopwords_list = [\"geehrte\", \"damen\", \"herren\", \"kollegen\", \"kolleginnen\", \"geehrter\", \"geehrten\", \"herr\", \"frau\"]\n",
    "\n",
    "for rede in speeches_19_20_prep:\n",
    "    words = rede['text'].split()\n",
    "    filtered_words = [word for word in words if word not in stopwords_list]\n",
    "    rede['text'] = ' '.join(filtered_words)"
   ],
   "id": "9458a25bcbb43ff9",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-31T12:05:03.378323Z",
     "start_time": "2024-08-31T11:35:10.620756Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#add the tokenized texts to data (for later use)\n",
    "for rede in speeches_19_20_prep:\n",
    "    doc = nlp(rede['text'])\n",
    "    rede.update({'text_token': []})\n",
    "    for token in doc:\n",
    "        rede['text_token'].append(token.text)"
   ],
   "id": "f90c4fdc51357adf",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-31T12:05:09.594384Z",
     "start_time": "2024-08-31T12:05:03.380361Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# save prepared data\n",
    "# Specify the file name\n",
    "file_name = 'data/speeches_19_20_prep.jsonl'\n",
    "\n",
    "# Open the file in write mode\n",
    "with open(file_name, 'w') as file:\n",
    "    # Write each dictionary as a JSON object followed by a newline\n",
    "    for item in speeches_19_20_prep:\n",
    "        json_line = json.dumps(item)\n",
    "        file.write(json_line + '\\n')"
   ],
   "id": "c2d51924d38d432e",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-02T10:01:45.569986Z",
     "start_time": "2024-09-02T10:01:40.167225Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# load prepared data\n",
    "speeches_19_20_prep = []\n",
    "with jsonlines.open('data/speeches_19_20_prep.jsonl') as f:\n",
    "    for line in f.iter():\n",
    "        #for line in list(f):\n",
    "        speeches_19_20_prep.append(line)"
   ],
   "id": "c0915d260b509a65",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-02T10:01:47.602567Z",
     "start_time": "2024-09-02T10:01:47.584339Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def build_co_occurrence_matrix(corpus, window_size, vocab_size):\n",
    "    word_counter = Counter()\n",
    "    for text in corpus:\n",
    "        word_counter.update(text['text_token'])\n",
    "\n",
    "    most_common_words = [word for word, _ in word_counter.most_common(vocab_size)]\n",
    "    unique_words = set(most_common_words)\n",
    "\n",
    "    co_occurrence_dict = defaultdict(lambda: defaultdict(float))\n",
    "\n",
    "    for text in corpus:\n",
    "        text_list = [word for word in text['text_token'] if word in unique_words]\n",
    "        for idx, word in enumerate(text_list):\n",
    "            i = max(0, idx - window_size)\n",
    "            j = min(len(text_list) - 1, idx + window_size)\n",
    "            search = [text_list[idx_] for idx_ in range(i, j + 1)]\n",
    "            search.remove(word)\n",
    "            for neighbor in search:\n",
    "                co_occurrence_dict[word][neighbor] += 1\n",
    "\n",
    "    return co_occurrence_dict"
   ],
   "id": "9c15a10f8d67101b",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-02T10:04:59.011433Z",
     "start_time": "2024-09-02T10:01:53.375180Z"
    }
   },
   "cell_type": "code",
   "source": "coo_dict=build_co_occurrence_matrix(speeches_19_20_prep,window_size=20,vocab_size=2500)",
   "id": "3ca3638a11b93748",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-02T10:06:20.292641Z",
     "start_time": "2024-09-02T10:06:16.872964Z"
    }
   },
   "cell_type": "code",
   "source": "coo_matrix = pd.DataFrame(coo_dict,index=coo_dict.keys()).fillna(0).astype(int)",
   "id": "3ea98e97221695a8",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-02T10:06:48.435973Z",
     "start_time": "2024-09-02T10:06:40.101720Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#calculate associations\n",
    "word_counter = Counter()\n",
    "for text in speeches_19_20_prep:\n",
    "    word_counter.update(text['text_token'])\n",
    "\n",
    "for column in coo_matrix.columns:\n",
    "    coo_matrix[column] = coo_matrix[column] * 10000 / (coo_matrix.index.map(word_counter) * word_counter[column])"
   ],
   "id": "aa4e1a828898fff2",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-02T10:07:08.677461Z",
     "start_time": "2024-09-02T10:07:08.539835Z"
    }
   },
   "cell_type": "code",
   "source": "coo_matrix = coo_matrix.astype(np.float32)",
   "id": "1a2db36b057ad808",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-02T10:07:13.900643Z",
     "start_time": "2024-09-02T10:07:13.624186Z"
    }
   },
   "cell_type": "code",
   "source": "coo_matrix.to_hdf('data/coo_matrix.h5', key='df', mode='w', complib='blosc', complevel=9)",
   "id": "c1f588b1fb0111c6",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "6da547987d87e2e0"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
